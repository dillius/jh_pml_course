<html>
	<body>
		<h2>Analysis of Data and Feature Creation</h2>
		<p>
			Upon analysis of the data the most immediate issue is the large number of covariates with almost no data at all in them. An initial analysis done with:
			<br />
				<pre>nsv = nearZeroVar(pmltraining,saveMetrics=TRUE)</pre>
			<br />
			Show a significant number varaibles with little actual data in them. More analysis by counting the number of NA and empty entries in each columns shows that many of these variables are not populated in even as much as 1% of the data:
			<br />
				<pre>colSums(is.na(training))</pre>
				<pre>colSums(training=="")</pre>
			<br />
			This provides us with some very simple cleanup we can perform on the covariates provided by removing these items that would not provide any useful input into the model. Our training set has 19217 records, so if we find any columns with greater than 19000 NA or empty values we remove them from consideration.
			<br />
				<pre>training1<-training[,colSums(!is.na(training))>19000]</pre>
				<pre>training2<-training1[,colSums(training1!="")>19000]</pre>
			<br />
			Looking at this reduced data set we also find a number of fields that are simply pointless to analyze: username, timestamps, and the like. We remove these values manually.
			<br />
				<pre>training3 <- training2[,8:60]</pre>
			<br />
			Finally, we look to determine highly correlated values in the training set so we can remove these somewhat duplicate pieces of information.
			<br />
				<pre>M <- abs(cor(training3[,-53]))</pre>
				<pre>diag(M) <- 0</pre>
				<pre>which(M > 0.8,arr.ind=T)</pre>
			<br />
			This provides us with a number of columns which we then concatenate into a list and remove from the training data
			<br />
				<pre>training4 <- training3[,!(names(training3) %in% drops)]</pre>
		</p>
		<h2>Training a Model</h2>
		<p>
			It has been recommended to use the Random Forest training method when training our models for the sake of performance. The normal training methods have taken some students an entire night of waiting to complete, which is impractical. Random Forest also has a high accuracy rate, which will benefit us in a scenario in which we need to get things right on the first try.
		<br />
			We will train a model using Random Forest for the method, with additional controls to include cross validation as a part of the training. First we specify these additional controls:
		<br />
			<pre>tc <- trainControl("cv", number=5) </pre>
		<br />
			We also perform some pre-processing on our remaining variables to center and scale the values we are going to use for training the model. We go ahead and apply the preprocessing to the testing data as well as the training, as this will need to be in this form to be run against our model later.
		<br />
			<pre>preProcValues <- preProcess(training4, method = c("center", "scale"))</pre>
			<pre>traininingTransformed <- predict(preProcValues, training4)</pre>
			<pre>testingTransformed <- predict(preProcValues, testing)</pre>
		<br />
			We then train our model using the Random Forest method and our now normalized data:
		<br />
			<pre>model <- train(classe ~., data=trainingTransformed, method="rf", trControl=tc)</pre>
		<br />
			Our model shows the following accuracy when cross validated:
<pre>
Random Forest

19622 samples
   23 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E'

No pre-processing
Resampling: Cross-Validated (5 fold)

Summary of sample sizes: 15699, 15697, 15699, 15696, 15697

Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD
   2    0.9772705  0.9712413  0.001157613  0.001468325
  13    0.9724287  0.9651197  0.001017273  0.001287926
  24    0.9657013  0.9566112  0.002476771  0.003138826

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2.
</pre>
		<br />
			We are led to expected a 97.7% Accuracy rating from this model.
		<br />
			Once our model is complete, we can predict the values from the testing data (remembering to use our preprocessed testing data):
		<br />
			<pre>answers <- predict(model, testingTransformed)</pre>
		<br />
			In the end when the twenty test problems are submitted we have correctly predicted 19/20, indicating a 95% accuracy rate in real use. Initial thoughts as to the failure to achieve a higher accuracy rate is likely due to having removed a highly correlated feature that may have provided a distinct point of analysis for this specific scenario.
		</p>
	</body>
</html>